{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from visualize import update_config, add_path\n",
    "\n",
    "lib_path = osp.join('lib')\n",
    "add_path(lib_path)\n",
    "\n",
    "import dataset as dataset\n",
    "from config import cfg\n",
    "import models\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'\n",
    "\n",
    "file_name = 'experiments/TP_H_w48_256x192_stage3_1_4_d96_h192_relu_enc6_mh1.yaml' # choose a yaml file\n",
    "f = open(file_name, 'r')\n",
    "update_config(cfg, file_name)\n",
    "\n",
    "model_name = 'T-H-A6'\n",
    "assert model_name in ['T-R', 'T-H','T-H-L','T-R-A4', 'T-H-A6', 'T-H-A5', 'T-H-A4' ,'T-R-A4-DirectAttention']\n",
    "\n",
    "normalize = T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "dataset = eval('dataset.' + cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TRAIN_SET, True,\n",
    "    transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      normalize,\n",
    "    ])\n",
    "  )\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=True\n",
    ")\n",
    "\n",
    "if cfg.TEST.MODEL_FILE:\n",
    "    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=True)\n",
    "else:\n",
    "    raise ValueError(\"please choose one ckpt in cfg.TEST.MODEL_FILE\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"model params:{:.3f}M\".format(sum([p.numel() for p in model.parameters()])/1000**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from core.inference import get_final_preds\n",
    "from utils import transforms, vis\n",
    "import cv2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tmp = []\n",
    "    tmp2 = []\n",
    "\n",
    "    img = dataset[0][0]\n",
    "\n",
    "    inputs = torch.cat([img.to(device)]).unsqueeze(0)\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(outputs, list):\n",
    "        output = outputs[-1]\n",
    "    else:\n",
    "        output = outputs\n",
    "        \n",
    "    preds, maxvals = get_final_preds(\n",
    "            cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "# from heatmap_coord to original_image_coord\n",
    "query_locations = np.array([p*4+0.5 for p in preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchvision\n",
    "\n",
    "def save_batch_image_with_joints(batch_image, batch_joints, file_name, nrow=8, padding=2):\n",
    "    '''\n",
    "    batch_image: [batch_size, channel, height, width]\n",
    "    batch_joints: [batch_size, num_joints, 3],\n",
    "    batch_joints_vis: [batch_size, num_joints, 1],\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    grid = torchvision.utils.make_grid(batch_image, nrow, padding, True)\n",
    "\n",
    "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "    ndarr = ndarr.copy()\n",
    "\n",
    "    nmaps = batch_image.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "\n",
    "    height = int(batch_image.size(1) + padding)\n",
    "    width = int(batch_image.size(2) + padding)\n",
    "\n",
    "    k = 0\n",
    "    for y in range(ymaps):\n",
    "        for x in range(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            joints = batch_joints[k]\n",
    "\n",
    "            print(batch_joints)\n",
    "\n",
    "            for idx, item in enumerate(batch_joints):\n",
    "                joint = batch_joints[idx]\n",
    "                joint[0] = x * width + padding + joint[0] # 加上 x * width + padding 的作用是：每个 gt.jpg 里的连续四张图片都能够展示对应的 gt 坐标\n",
    "                joint[1] = y * height + padding + joint[1]\n",
    "\n",
    "                color = [28, 1, 255]\n",
    "\n",
    "                if idx >= 0 and idx <= 4:\n",
    "                    color = [28, 1, 255]\n",
    "                elif idx >=5 and idx <= 8:\n",
    "                    color = [22, 128, 0]\n",
    "                elif idx >= 9 and idx <= 12:\n",
    "                    color = [62, 255, 255]\n",
    "                elif idx >= 13 and idx <= 16:\n",
    "                    color = [254, 255, 0]\n",
    "                elif idx >= 17 and idx <= 20:\n",
    "                    color = [203, 192, 255]\n",
    "                \n",
    "                cv2.circle(ndarr, (int(joint[0]), int(\n",
    "                    joint[1])), 1, color, 2)\n",
    "            k = k + 1\n",
    "    cv2.imwrite(file_name, ndarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{}_gt.jpg'.format('demo')\n",
    "\n",
    "save_batch_image_with_joints(img, query_locations, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('my_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfe586b9d7864ba3e4264622bc2b1a378a49721f9561084f849ca2c6573a7ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
